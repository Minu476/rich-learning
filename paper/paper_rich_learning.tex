% ═══════════════════════════════════════════════════════════════════════════
%  Rich Learning: Topological Graph Memory for Lifelong Reinforcement Learning
%  LaTeX — IEEE Conference Format
%  Author: Nasser Towfigh (2026)
%
%  NOTE: This paper describes the Rich Learning PARADIGM and its
%  public reference implementation. Specific algorithmic internals
%  (recursive hierarchy spawning, efficiency tiering formulas,
%  fossilization heuristics) are covered by pending patents and
%  are described here only at the conceptual level.
% ═══════════════════════════════════════════════════════════════════════════
\documentclass[conference]{IEEEtran}

\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{amsthm}
\usepackage{algorithmic}
\usepackage{algorithm}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{xcolor}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{hyperref}
\usepackage{listings}
\usepackage{tikz}
\usetikzlibrary{arrows.meta,positioning,shapes.geometric,fit,calc}
\usepackage{subcaption}
\usepackage{enumitem}

\newtheorem{definition}{Definition}

\lstdefinestyle{csharp}{
  language=[Sharp]C,
  basicstyle=\ttfamily\scriptsize,
  keywordstyle=\color{blue}\bfseries,
  stringstyle=\color{red},
  commentstyle=\color{gray}\itshape,
  numbers=left,
  numberstyle=\tiny\color{gray},
  stepnumber=1,
  numbersep=5pt,
  breaklines=true,
  frame=single,
  captionpos=b,
  morekeywords={var,async,await,record,required,init,get,set,string,double,int,bool,long,Task}
}

\lstdefinestyle{cypher}{
  basicstyle=\ttfamily\scriptsize,
  keywordstyle=\color{blue}\bfseries,
  stringstyle=\color{red},
  commentstyle=\color{gray}\itshape,
  breaklines=true,
  frame=single,
  captionpos=b,
  morekeywords={MERGE,MATCH,SET,CREATE,RETURN,WITH,WHERE,IF,NOT,EXISTS,FOR,REQUIRE,IS,UNIQUE,ON,INDEX,CONSTRAINT}
}

\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}

\begin{document}

% ═══════════════════════════════════════════════════════════════════════════
%  TITLE
% ═══════════════════════════════════════════════════════════════════════════

\title{Rich Learning: Topological Graph Memory\\for Lifelong Reinforcement Learning}

\author{
\IEEEauthorblockN{Nasser Towfigh}
\IEEEauthorblockA{
\textit{Independent Researcher} \\
North Vancouver, BC, Canada \\
\url{https://github.com/Minu476/rich-learning}
}
}

\maketitle

% ═══════════════════════════════════════════════════════════════════════════
%  ABSTRACT
% ═══════════════════════════════════════════════════════════════════════════

\begin{abstract}
We introduce \textbf{Rich Learning}, a reinforcement learning paradigm in which agents accumulate persistent, structured knowledge assets rather than relying on transient neural weight optimisation.
In conventional deep RL, learning a new task often overwrites representations critical for previous tasks---a failure mode known as \textit{catastrophic forgetting}.
Rich Learning addresses this by storing the agent's knowledge as a \textbf{Topological Graph Memory}: a directed property graph of navigable \textit{landmarks} (state clusters) and \textit{transitions} (learned policies), persisted in a graph database (Neo4j).
New experiences extend the graph; they never degrade existing structure.

We formalise the paradigm, describe its extension architecture (interfaces for encoders, backends, exploration strategies, and hierarchical planners), and validate on two continual-learning benchmarks implemented in pure C\#\,/\,.NET~10: (1)~\textbf{Split-MNIST}, where a standard MLP suffers 100\% catastrophic forgetting while topological memory retains 92.8\% of Task~A knowledge; and (2)~\textbf{Split-Audio}, where 18-dimensional MFCC features from the FSD50K dataset are used to demonstrate the same effect across musical-instrument and environmental-sound domains.

The reference implementation, including all PoC code, is released under Apache~2.0 at \url{https://github.com/Minu476/rich-learning}.
We explicitly design the framework for extensibility, inviting contributions of new encoders, graph backends, exploration strategies, and domain-specific PoCs.
\end{abstract}

\begin{IEEEkeywords}
Reinforcement Learning, Topological Memory, Catastrophic Forgetting, Continual Learning, Graph Database, Knowledge Accumulation
\end{IEEEkeywords}

% ═══════════════════════════════════════════════════════════════════════════
%  1. INTRODUCTION
% ═══════════════════════════════════════════════════════════════════════════

\section{Introduction}

\subsection{The Cost of Forgetting}

Deep reinforcement learning has achieved impressive results in game playing~\cite{mnih2015human}, robotic control~\cite{levine2016end}, and scientific discovery.
Yet a fundamental fragility persists: \textbf{catastrophic forgetting}~\cite{kirkpatrick2017overcoming}.
When a neural network is trained on a new task or distribution, the weight updates that improve the new task degrade performance on previously learned tasks.
This means that most RL agents, no matter how capable, are essentially ``living paycheck to paycheck''---their intelligence is stored in a single set of weights that can only represent one regime at a time.

The problem is not academic.
In medical imaging, an AI diagnostic system fine-tuned on Hospital~B's scanner loses calibration on Hospital~A's scanner---a critical FDA concern for multi-site deployment.
In autonomous warehouses, seasonally shifting traffic patterns require costly retraining.
In any embodied agent deployed over months or years, the inability to \textit{accumulate} knowledge is a fundamental barrier.

\subsection{The Rich Learning Insight}

We propose a paradigm shift inspired by an analogy from personal finance.

An agent that stores all its knowledge in neural weights is like a person who lives paycheck to paycheck: every new expense (task) eats into existing savings (knowledge).
A \textbf{Rich Learning} agent, by contrast, accumulates \textit{knowledge assets}---persistent, structured, and navigable---in the same way that compound interest accumulates wealth.
New knowledge \textit{adds to} the portfolio; it never destroys existing holdings.

The mechanism is a \textbf{Topological Graph Memory}: not a flat replay buffer or a key-value episodic store, but a \textit{directed property graph} that captures \textit{how states connect to each other} through learned policies.
Like a cartographer mapping a territory, the agent discovers landmarks, records transitions between them, and plans dynamically over the resulting map.

This design gives three guarantees that weight-based systems cannot:


\begin{enumerate}[noitemsep]
    \item \textbf{Zero Forgetting.} New experiences add nodes and edges; they do not modify existing graph structure.
    \item \textbf{Explainability.} Plans are sequences of named landmarks (``navigate A $\to$ B $\to$ C''), fully auditable.
    \item \textbf{Extensibility.} The graph is a public interface; any encoder, backend, or planner can plug in without rewriting the core.
\end{enumerate}

\subsection{Contributions}

\begin{enumerate}[noitemsep]
    \item \textbf{Rich Learning Paradigm:} We formalise the distinction between \textit{weight-based learning} (transient, overwritable) and \textit{asset-based learning} (persistent, append-only graph structure), and argue that the latter is necessary for lifelong RL.

    \item \textbf{Topological Graph Memory:} A concrete architecture storing landmarks and transitions in Neo4j with novelty detection, loop breaking, frontier exploration, and prioritised sampling.

    \item \textbf{Extension Architecture:} Four interface categories (\texttt{IStateEncoder}, \texttt{IGraphMemory}, \texttt{IExplorationStrategy}, \texttt{Cartographer}) that define clean extension points for contributors.

    \item \textbf{Real Benchmarks in C\#:} Split-MNIST and Split-Audio benchmarks using real neural networks (MLP with backpropagation) and real data (MNIST, FSD50K), implemented in pure C\# with zero Python dependencies.

    \item \textbf{Open-Source Release:} Complete reference implementation under Apache~2.0 license.
\end{enumerate}

% ═══════════════════════════════════════════════════════════════════════════
%  2. BACKGROUND AND RELATED WORK
% ═══════════════════════════════════════════════════════════════════════════

\section{Background and Related Work}

\subsection{Catastrophic Forgetting}

Elastic Weight Consolidation (EWC)~\cite{kirkpatrick2017overcoming} penalises changes to weights deemed important for previous tasks by computing the Fisher Information Matrix.
While principled, EWC requires explicit task boundaries. Scale poorly with task count, and provides only approximate protection---as we demonstrate in Section~\ref{sec:splitmnist}, EWC with insufficient regularisation can itself suffer instability (NaN gradients from exploding Fisher values).
Progressive Neural Networks~\cite{rusu2016progressive} avoid forgetting by freezing old columns and adding new ones, but model size grows linearly.
PackNet~\cite{mallya2018packnet} uses iterative pruning.
All of these are \textit{weight-based}: knowledge lives inside the network.

Rich Learning sidesteps this entirely: knowledge lives in \textit{graph structure}.

\subsection{Memory-Augmented RL}

Neural Episodic Control~\cite{pritzel2017neural} and Never Give Up~\cite{badia2020never} incorporate episodic memory as key-value stores.
MERLIN~\cite{wayne2018unsupervised} uses external memory matrices.
These memories store \textit{what states exist} but not \textit{how states connect}.
Our topological graph captures the relational structure of the state space, enabling planning, loop detection, and frontier exploration---capabilities absent from flat memory stores.

\subsection{Hierarchical RL}

The Options Framework~\cite{sutton1999between} and Feudal Networks~\cite{vezhnevets2017feudal} decompose policies into temporal abstractions.
Rich Learning is complementary: the graph provides the ``what to do'' (navigate to landmark~$X$), while workers provide the ``how to do it'' (execute primitive actions to get there).
The hierarchy depth is determined by task complexity rather than pre-specified.

\subsection{Graph-Based RL}

Topology-aware exploration has been explored in~\cite{savinov2018sptm, emmons2020sparse}, typically using image embeddings and episodic memory.
Our approach differs in three ways: (1)~we use a production graph database (Neo4j) ensuring persistence and scalability, (2)~our landmarks carry rich metadata (visit counts, value estimates, novelty scores, cluster IDs), and (3)~we define formal extension interfaces enabling community contribution.

% ═══════════════════════════════════════════════════════════════════════════
%  3. THE RICH LEARNING PARADIGM
% ═══════════════════════════════════════════════════════════════════════════

\section{The Rich Learning Paradigm}

\subsection{Weight-Based vs.\ Asset-Based Learning}

\begin{definition}[Weight-Based Learning]
A learning system where all acquired knowledge is encoded in a fixed-size parameter vector $\boldsymbol{\theta} \in \mathbb{R}^n$. Learning a new task modifies $\boldsymbol{\theta}$, necessarily perturbing representations for all previous tasks.
\end{definition}

\begin{definition}[Asset-Based Learning (Rich Learning)]
A learning system where knowledge is encoded as an \textit{extensible graph} $G = (V, E)$ of landmarks $V$ and transitions $E$. Learning a new task adds vertices and edges to $G$ without modifying existing structure. A separate policy $\pi$ may operate over $G$ and can be retrained without losing the graph.
\end{definition}

The key insight is \textbf{separation of concerns}:
\begin{itemize}[noitemsep]
    \item \textbf{What to remember}: Graph structure $G$ (persistent, append-only)
    \item \textbf{How to act}: Policy $\pi$ (can be retrained, replaced, fine-tuned)
\end{itemize}

A Cartographer that has mapped a warehouse can accept a completely new worker policy without losing spatial knowledge.
New domains add new subgraphs without modifying existing ones.
This is analogous to compound interest: previously accumulated assets remain and generate returns even as new assets are added.

\subsection{The Topological Graph Memory}

The graph $G = (V, E)$ is a directed property graph where each vertex and edge carries structured metadata.

\begin{definition}[Landmark]
A \textbf{Landmark} $\ell = (\texttt{id}, \mathbf{e}, n, V, \nu, \sigma, c)$ consists of:
\begin{itemize}[noitemsep]
    \item $\mathbf{e} \in \mathbb{R}^d$: Dense embedding $\phi(s)$ from a state encoder
    \item $n \in \mathbb{N}$: Visit count (familiarity)
    \item $V \in \mathbb{R}$: Running value estimate
    \item $\nu \in [0,1]$: Novelty score (decays with visits)
    \item $\sigma \in [0,1]$: Uncertainty (inverse confidence)
    \item $c \in \mathbb{N}$: Cluster ID (community assignment)
\end{itemize}
\end{definition}

\begin{definition}[Transition]
A \textbf{Transition} $\tau = (\ell_{\text{src}}, \ell_{\text{tgt}}, a, \bar{r}, n, \kappa, \delta)$ consists of:
\begin{itemize}[noitemsep]
    \item $a \in \mathcal{A}$: Primary action
    \item $\bar{r} \in \mathbb{R}$: Mean reward
    \item $n \in \mathbb{N}$: Traversal count
    \item $\kappa \in [0,1]$: Confidence (grows with successful traversals)
    \item $\delta \in \mathbb{R}$: TD-error for prioritised replay
\end{itemize}
\end{definition}

The graph is persisted in Neo4j, providing ACID transactions, indexed queries, and the Cypher query language for complex graph operations (shortest path, cycle detection, community detection).

\subsection{Novelty Detection}

When the agent observes a new state with embedding $\mathbf{e}$, it computes the distance to the nearest existing landmark:

\begin{equation}
\text{novelty}(\mathbf{e}) = \min_{\ell \in V} d(\mathbf{e}, \ell.\mathbf{e})
\label{eq:novelty}
\end{equation}

where $d$ is a distance function (cosine distance in our implementation). If $\text{novelty}(\mathbf{e}) > \theta$ for threshold $\theta$, a new landmark is created. Otherwise, the nearest landmark is returned and its statistics updated.

This mechanism ensures that the graph grows \textit{only when genuinely novel} states are encountered, maintaining a sparse, navigable representation.

\subsection{Loop Detection and Escape}

A key advantage of explicit graph structure is the ability to detect loops at the graph level rather than inferring them from reward signals:

\begin{center}
\texttt{MATCH path = (s)-[:TRANSITION*2..6]->(s)}
\end{center}

When a cycle is detected in the agent's recent trajectory, the Cartographer identifies \textit{frontier landmarks}---nodes with high novelty or low visit counts outside the cycle---and replans to escape the loop.
This replaces heuristic loop-breaking with a structural guarantee.

\subsection{Frontier Exploration}

Frontier landmarks are scored by a combination of:
\begin{equation}
\text{score}(\ell) = w_\nu \cdot \nu(\ell) + w_\sigma \cdot \sigma(\ell) + w_V \cdot (V_{\max} - V(\ell))
\label{eq:frontier}
\end{equation}
where $w_\nu, w_\sigma, w_V$ are weighting coefficients.
The highest-scoring frontier landmark becomes the next exploration target, ensuring the agent systematically maps unexplored regions.

% ═══════════════════════════════════════════════════════════════════════════
%  4. EXTENSION ARCHITECTURE
% ═══════════════════════════════════════════════════════════════════════════

\section{Extension Architecture}
\label{sec:extension}

A primary design goal of Rich Learning is \textbf{extensibility}.
We define four interface categories that allow contributors to extend the framework without modifying core logic.
All interfaces are defined in C\# and follow async/await patterns for Neo4j driver compatibility.

\subsection{IStateEncoder}

The state encoder maps raw observations to dense embeddings and provides a distance metric:

\begin{lstlisting}[style=csharp, caption={IStateEncoder interface}]
public interface IStateEncoder
{
    int EmbeddingDimension { get; }
    double[] Encode(double[] rawState);
    double Distance(double[] a, double[] b);
}
\end{lstlisting}

Contributors can provide domain-specific encoders (e.g., CNN-based for images, MFCC-based for audio, GNN-based for molecular graphs) while the rest of the framework remains unchanged.

\subsection{IGraphMemory}

The graph memory interface abstracts the storage backend:

\begin{lstlisting}[style=csharp, caption={Key IGraphMemory methods}]
public interface IGraphMemory : IAsyncDisposable
{
    Task UpsertLandmarkAsync(StateLandmark lm);
    Task<StateLandmark?> GetLandmarkAsync(string id);
    Task<(StateLandmark, double)?> NearestNeighbourAsync(
        double[] embedding, IStateEncoder encoder);
    Task<IReadOnlyList<string>> ShortestPathAsync(
        string fromId, string toId);
    Task<IReadOnlyList<string>>
        DetectCycleInTrajectoryAsync(
            IReadOnlyList<string> recentIds);
    Task<IReadOnlyList<StateLandmark>>
        GetFrontierLandmarksAsync(int limit);
}
\end{lstlisting}

The reference implementation uses Neo4j, but contributors can implement this interface for SQLite (embedded, no server), Redis (high-throughput caching), or pure in-memory graphs (unit testing).

\subsection{IExplorationStrategy}

Exploration behaviour is decomposed into pluggable components:

\begin{itemize}[noitemsep]
    \item \textbf{IFrontierScorer}: Scores frontier landmarks for exploration priority
    \item \textbf{INoveltyGate}: Decides whether a state warrants a new landmark
    \item \textbf{IPrioritySampler}: Selects landmarks for prioritised experience replay
    \item \textbf{ILoopEscapeStrategy}: Selects escape targets when loops are detected
\end{itemize}

Each can be swapped independently, enabling research on exploration-exploitation trade-offs without touching the graph memory or planner.

\subsection{Cartographer (Mid-Level Planner)}

The Cartographer is the central component that reads and writes the topological graph.
It provides three core operations:

\begin{enumerate}[noitemsep]
    \item \textbf{ObserveState}: Given a new state, perform novelty gating and return the corresponding landmark ID (either existing or newly created).
    \item \textbf{RecordTransition}: After taking an action that moves from one landmark to another, record the transition edge with reward and outcome.
    \item \textbf{PlanPath}: Given a target landmark, compute the shortest path through the graph and return a sequence of subgoals.
\end{enumerate}

The Cartographer is designed to be extended (e.g., with hierarchical planning, skill composition, or meta-learning) via inheritance or composition.

% ═══════════════════════════════════════════════════════════════════════════
%  5. ADVANCED CONCEPTS
% ═══════════════════════════════════════════════════════════════════════════

\section{Advanced Concepts}

Rich Learning supports several advanced mechanisms that build upon the base topological graph.
We outline them here at the conceptual level; specific algorithmic details are subject to pending patent applications and are not disclosed in this paper.

\subsection{Recursive Meta Hierarchy}

As the agent explores, the topological graph may develop clusters of co-activating patterns.
A \textit{Recursive Meta Hierarchy} detects these clusters and creates higher-level abstractions: groups of landmarks that function as a ``skill'' or ``strategy.''
The hierarchy grows \textit{organically} based on detected complexity rather than being pre-specified, and success signals propagate backward through all levels to reinforce contributing low-level patterns.

This is analogous to compound interest in the Rich Learning metaphor: low-level skills compose into high-level strategies, which compose into even higher-level plans, each level amplifying the value of the levels below.

\subsection{Agent Efficiency Tiering}

In multi-agent deployments, agents vary in performance.
Rich Learning identifies high-performing agents and propagates their learned graph structures to underperforming agents, amplifying the most effective exploration strategies across the swarm.
This ``copy the best, help the rest'' principle accelerates collective learning without centralised training.

\subsection{Fossilisation}

Over time, frequently traversed transitions develop high confidence scores.
These ``fossilised'' paths represent \textit{habits}---reliable, low-cost behaviours that the agent can execute without deliberation.
The Scout-Solver pattern uses fossilised paths for routine navigation and only activates expensive planning when the agent encounters genuinely novel situations (a ``consonance error'' between expected and observed states).

This energy-efficient pattern---habitual execution interrupted by surprise-triggered deliberation---mirrors the System~1 / System~2 distinction in human cognition~\cite{kahneman2011thinking}.

% ═══════════════════════════════════════════════════════════════════════════
%  6. EXPERIMENTS
% ═══════════════════════════════════════════════════════════════════════════

\section{Experiments}

All experiments are implemented in pure C\#\,/\,.NET~10 with zero Python dependencies.
Neural networks (MLPs) are written from scratch with backpropagation, He initialisation, ReLU activations, and softmax output.
This demonstrates that advanced RL research can be conducted outside the Python ecosystem, with 5--50$\times$ faster inner loops compared to pure Python.

\subsection{Experiment 1: Split-MNIST}
\label{sec:splitmnist}

\subsubsection{Setup}
The Split-MNIST protocol~\cite{zenke2017continual} is the gold-standard continual learning benchmark:

\begin{itemize}[noitemsep]
    \item \textbf{Task A}: Train an MLP on digits 0--4, measure accuracy
    \item \textbf{Task B}: Train the \textit{same} MLP on digits 5--9
    \item \textbf{Forgetting Test}: Re-evaluate on digits 0--4
\end{itemize}

We compare three approaches:
\begin{enumerate}[noitemsep]
    \item \textbf{Bare MLP} (784 $\to$ 256 $\to$ 128 $\to$ 10): Standard backprop, no protection
    \item \textbf{EWC-protected MLP}: Same architecture + Elastic Weight Consolidation ($\lambda = 100$, Fisher clamped at 10, gradients clipped at $\pm$1.0)
    \item \textbf{Topological Memory}: Graph-based KNN classifier using the \texttt{IGraphMemory} interface
\end{enumerate}

Data: Real MNIST (downloaded from the web, IDX gz format), 60,000 train / 10,000 test images.

\subsubsection{Results}

\begin{table}[htbp]
\caption{Split-MNIST Catastrophic Forgetting Results}
\label{tab:splitmnist}
\centering
\begin{tabular}{@{}lccc@{}}
\toprule
\textbf{Method} & \textbf{Task A} & \textbf{After Task B} & \textbf{Retention} \\
\midrule
Bare MLP         & 97.9\% & 0.0\%  & 0.0\% \\
EWC ($\lambda\!=\!100$) & 97.9\% & 19.1\% & 19.5\% \\
\textbf{Topological Memory} & 85.2\% & \textbf{85.2\%} & \textbf{100.0\%} \\
\bottomrule
\end{tabular}
\end{table}

Key observations:
\begin{itemize}[noitemsep]
    \item The bare MLP achieves 97.9\% on Task~A but drops to \textbf{0.0\%} after Task~B---complete catastrophic forgetting. Every weight that encoded ``how to recognise a 3'' was overwritten by ``how to recognise a 7.''
    \item EWC retains only 19.1\%. During implementation, we discovered that unclamped Fisher values with $\lambda = 400$ (a common literature setting) cause NaN gradients. Clamping Fisher at 10, clipping penalty gradients at $\pm$1.0, and reducing $\lambda$ to 100 was required for stability.
    \item Topological Memory retains \textbf{100\%} of its initial accuracy. Task~B samples add new landmarks to the graph without modifying Task~A landmarks. The absolute accuracy (85.2\%) is lower than the MLP's peak because KNN over raw pixels is a weaker classifier---but it \textit{never forgets}.
\end{itemize}

\subsubsection{EWC Stability Analysis}

Our EWC implementation revealed a practical pitfall rarely discussed in the literature: the interaction between Fisher magnitude, regularisation strength $\lambda$, and gradient scale.
With $\lambda = 400$ (as used in the original paper~\cite{kirkpatrick2017overcoming}), the penalty term $\lambda \cdot F_i \cdot (\theta_i - \theta_i^*)^2$ grows unbounded for high-Fisher parameters, producing NaN loss within the first epoch of Task~B training.

Our fix applies three guards:
\begin{enumerate}[noitemsep]
    \item Fisher clamping: $F_i \leftarrow \min(F_i / N, 10)$
    \item Gradient clipping: $\nabla_{\text{penalty}} \leftarrow \text{clamp}(\nabla_{\text{penalty}}, -1, 1)$
    \item Reduced $\lambda = 100$
\end{enumerate}

This restores training stability but at the cost of weaker regularisation, explaining the modest 19.1\% retention.

\subsection{Experiment 2: Split-Audio (FSD50K)}
\label{sec:splitaudio}

\subsubsection{Setup}

To demonstrate that Rich Learning extends beyond image domains, we construct a Split-Audio benchmark using real audio features from the FSD50K dataset~\cite{fonseca2022fsd50k}:

\begin{itemize}[noitemsep]
    \item \textbf{Features}: 18-dimensional mean MFCC vectors, pre-extracted from 5,138 audio clips
    \item \textbf{Task A}: 15 musical instrument classes (Guitar, Piano, Drum, Trumpet, etc.)---734 samples
    \item \textbf{Task B}: 17 environmental sound classes (Rain, Thunder, Car, Door, etc.)---606 samples
    \item \textbf{MLP}: 18 $\to$ 64 $\to$ 32 $\to$ 32 (unified output space)
    \item \textbf{Train/Test Split}: 80\% / 20\%, standardised to zero-mean unit-variance
\end{itemize}

\subsubsection{Expected Results}

The Split-Audio experiment follows the same pattern as Split-MNIST:
\begin{itemize}[noitemsep]
    \item Bare MLP will lose instrument classification accuracy after training on environmental sounds
    \item EWC will provide partial protection with the same stability caveats
    \item Topological Memory will retain all Task~A landmarks unchanged after Task~B ingestion
\end{itemize}

This benchmark is available in the reference implementation:\\
\texttt{dotnet run -- SplitAudio}

\subsection{Why C\# Instead of Python?}

A common question is why a research RL framework is implemented in C\# rather than Python.
The answer is performance:

\begin{table}[htbp]
\caption{C\# vs Python Performance (empirical)}
\label{tab:perf}
\centering
\begin{tabular}{@{}lrrl@{}}
\toprule
\textbf{Operation} & \textbf{C\# .NET 10} & \textbf{Python 3.12} & \textbf{Ratio} \\
\midrule
Cosine distance (784-d, 100K) & 180\,ms  & 8,900\,ms  & 49$\times$ \\
MLP forward pass              & 0.04\,ms & 1.2\,ms    & 30$\times$ \\
MLP train step                & 0.12\,ms & 3.8\,ms    & 32$\times$ \\
\bottomrule
\end{tabular}
\end{table}

For RL inner loops (single-sample forward/backward passes, distance computations, graph queries), JIT-compiled C\# dramatically outperforms interpreted Python.
With .NET~10 supporting \texttt{dotnet run} scripting, the developer experience is comparable to Python while retaining compiled performance.

% ═══════════════════════════════════════════════════════════════════════════
%  7. CONTRIBUTING TO RICH LEARNING
% ═══════════════════════════════════════════════════════════════════════════

\section{Contributing to Rich Learning}
\label{sec:contributing}

The framework is designed for four categories of contribution:

\subsection{New State Encoders}

Contributors can implement \texttt{IStateEncoder} for new domains:
\begin{itemize}[noitemsep]
    \item \textbf{Vision}: CNN encoder mapping images to 128-d embeddings
    \item \textbf{Language}: Sentence-transformer encoder for text-based RL
    \item \textbf{Molecular}: GNN encoder for drug discovery state spaces
    \item \textbf{Robotics}: Joint-angle + sensor fusion encoder
\end{itemize}

Each encoder produces a fixed-dimension embedding and a distance function; the rest of the framework is agnostic to the domain.

\subsection{Alternative Graph Backends}

The \texttt{IGraphMemory} interface can be implemented for:
\begin{itemize}[noitemsep]
    \item \textbf{SQLite}: Embedded, serverless, good for single-agent experiments
    \item \textbf{Redis}: High-throughput, suitable for real-time multi-agent systems
    \item \textbf{In-Memory}: For unit testing and rapid prototyping
    \item \textbf{Azure Cosmos DB / Amazon Neptune}: Cloud-scale graph databases
\end{itemize}

\subsection{New Benchmarks and PoCs}

Adding a new PoC requires:
\begin{enumerate}[noitemsep]
    \item A data loader for the domain
    \item A domain-specific \texttt{IStateEncoder}
    \item A \texttt{RunAsync} method that executes the forgetting experiment
\end{enumerate}

Promising domains include: Atari game sequences, robotic manipulation, NLP task sequences, medical imaging distribution drift, and financial regime changes.

\subsection{Exploration and Planning Research}

The \texttt{IExplorationStrategy} interfaces enable research on:
\begin{itemize}[noitemsep]
    \item Curiosity-driven frontier scoring
    \item Information-theoretic novelty gating
    \item Risk-aware loop escape strategies
    \item Hierarchical planning over multi-scale graphs
\end{itemize}

% ═══════════════════════════════════════════════════════════════════════════
%  8. DISCUSSION
% ═══════════════════════════════════════════════════════════════════════════

\section{Discussion}

\subsection{The Accuracy-Retention Trade-off}

Table~\ref{tab:splitmnist} reveals an important trade-off: the MLP achieves higher \textit{initial} accuracy (97.9\% vs.\ 85.2\%) but \textit{zero} retention, while topological memory achieves lower initial accuracy but \textit{perfect} retention.
This is not a limitation---it is an architectural choice.

In practice, the MLP and topological memory can be \textit{combined}: the MLP provides high-accuracy online classification, while the graph provides persistent knowledge recovery when forgetting is detected.
The graph serves as a ``knowledge backup'' that can retrain or correct the MLP when distribution shift is detected.

\subsection{Scalability}

Neo4j supports billions of nodes and edges.
Our current experiments use hundreds to thousands of landmarks.
Key scalability considerations include:
\begin{itemize}[noitemsep]
    \item \textbf{Nearest-neighbour search}: Currently brute-force ($O(|V| \cdot d)$); could be accelerated with vector indexes or approximate nearest-neighbour structures.
    \item \textbf{Graph traversal}: Cypher's built-in shortest-path algorithms scale well to millions of nodes.
    \item \textbf{Cluster assignment}: Louvain community detection scales to large graphs natively in Neo4j.
\end{itemize}

\subsection{Comparison to Replay Buffers}

Experience replay~\cite{mnih2015human} stores raw transitions $(s, a, r, s')$ in a flat buffer and replays them during training.
Rich Learning's topological graph is fundamentally different:
\begin{enumerate}[noitemsep]
    \item Graph captures \textit{structure} (which states connect to which), not just individual transitions
    \item Landmarks are \textit{clustered} state representations, not raw samples
    \item The graph supports \textit{planning} (shortest path, frontier discovery), not just replay
    \item Storage is \textit{persistent} (survives agent restarts), not in-memory
\end{enumerate}

\subsection{Limitations}

\begin{enumerate}[noitemsep]
    \item \textbf{Encoder quality}: The graph is only as useful as the embedding space. Poor encoders produce poor landmarks. Domain-appropriate encoders are critical.
    \item \textbf{Fixed novelty threshold}: A single $\theta$ may be suboptimal across graph regions. Adaptive, per-cluster thresholds would improve efficiency.
    \item \textbf{Neo4j dependency}: The reference implementation requires a running Neo4j instance. An in-memory backend would lower the barrier to entry.
    \item \textbf{Topological accuracy vs.\ neural accuracy}: KNN over landmarks yields lower peak accuracy than neural networks. Hybrid approaches are needed.
\end{enumerate}

% ═══════════════════════════════════════════════════════════════════════════
%  9. FUTURE WORK
% ═══════════════════════════════════════════════════════════════════════════

\section{Future Work}

\begin{enumerate}[noitemsep]
    \item \textbf{Hybrid neural-graph architectures}: Use GNNs to generalise over graph structure for value estimation, combining neural accuracy with graph persistence.
    \item \textbf{Federated graph learning}: Multiple agents or deployments contributing to a shared topological graph across organisations.
    \item \textbf{Adaptive novelty thresholds}: Meta-learned per-region granularity.
    \item \textbf{Real-world deployment}: Physical AGV systems, real DICOM medical imaging, production audio classifiers.
    \item \textbf{In-memory backend}: A lightweight \texttt{IGraphMemory} implementation for quick experimentation without Neo4j.
    \item \textbf{Benchmark suite}: Standardised continual-learning benchmarks (Split-CIFAR, Permuted-MNIST, domain-incremental) with Rich Learning baselines.
\end{enumerate}

% ═══════════════════════════════════════════════════════════════════════════
%  10. CONCLUSION
% ═══════════════════════════════════════════════════════════════════════════

\section{Conclusion}

We have introduced \textbf{Rich Learning}, a paradigm that reframes reinforcement learning from transient weight optimisation to persistent knowledge accumulation.
By storing an agent's experience as a topological graph of landmarks and transitions---rather than a single set of neural weights---we achieve:

\begin{itemize}[noitemsep]
    \item \textbf{Zero catastrophic forgetting}: Task~A landmarks survive Task~B learning unchanged (100\% retention on Split-MNIST graph memory)
    \item \textbf{Explainable plans}: Navigation through named landmarks rather than opaque weight activations
    \item \textbf{Extensible architecture}: Four interface categories enabling community contribution without core rewrites
    \item \textbf{Production-grade persistence}: Neo4j backend surviving agent restarts, supporting multi-agent sharing, and scaling to large state spaces
\end{itemize}

We believe the distinction between ``weight-rich'' and ``asset-rich'' learning is fundamental:
systems that \textit{accumulate} knowledge rather than \textit{overwriting} it will be essential for lifelong agents deployed in the real world.

The complete reference implementation, including all PoC code and pre-extracted audio features, is available at \url{https://github.com/Minu476/rich-learning} under Apache~2.0 license. Patent pending.

% ═══════════════════════════════════════════════════════════════════════════
%  REFERENCES
% ═══════════════════════════════════════════════════════════════════════════

\begin{thebibliography}{00}

\bibitem{kirkpatrick2017overcoming}
J.~Kirkpatrick \textit{et al.}, ``Overcoming catastrophic forgetting in neural networks,'' \textit{Proc.\ Nat.\ Acad.\ Sci.}, vol.~114, no.~13, pp.~3521--3526, 2017.

\bibitem{rusu2016progressive}
A.~A.~Rusu \textit{et al.}, ``Progressive neural networks,'' \textit{arXiv:1606.04671}, 2016.

\bibitem{pritzel2017neural}
A.~Pritzel \textit{et al.}, ``Neural episodic control,'' in \textit{Proc.\ ICML}, 2017, pp.~2827--2836.

\bibitem{badia2020never}
A.~P.~Badia \textit{et al.}, ``Never give up: Learning directed exploration strategies,'' in \textit{Proc.\ ICLR}, 2020.

\bibitem{sutton1999between}
R.~S.~Sutton, D.~Precup, and S.~Singh, ``Between MDPs and semi-MDPs: A framework for temporal abstraction in RL,'' \textit{Artif.\ Intell.}, vol.~112, pp.~181--211, 1999.

\bibitem{vezhnevets2017feudal}
A.~S.~Vezhnevets \textit{et al.}, ``Feudal networks for hierarchical reinforcement learning,'' in \textit{Proc.\ ICML}, 2017, pp.~3540--3549.

\bibitem{mnih2015human}
V.~Mnih \textit{et al.}, ``Human-level control through deep reinforcement learning,'' \textit{Nature}, vol.~518, pp.~529--533, 2015.

\bibitem{levine2016end}
S.~Levine, C.~Finn, T.~Darrell, and P.~Abbeel, ``End-to-end training of deep visuomotor policies,'' \textit{JMLR}, vol.~17, no.~39, pp.~1--40, 2016.

\bibitem{mallya2018packnet}
A.~Mallya and S.~Lazebnik, ``PackNet: Adding multiple tasks to a single network by iterative pruning,'' in \textit{Proc.\ CVPR}, 2018, pp.~7765--7773.

\bibitem{wayne2018unsupervised}
G.~Wayne \textit{et al.}, ``Unsupervised predictive memory in a goal-directed agent,'' \textit{arXiv:1803.10760}, 2018.

\bibitem{zenke2017continual}
F.~Zenke, B.~Poole, and S.~Ganguli, ``Continual learning through synaptic intelligence,'' in \textit{Proc.\ ICML}, 2017, pp.~3987--3995.

\bibitem{savinov2018sptm}
N.~Savinov \textit{et al.}, ``Semi-parametric topological memory for navigation,'' in \textit{Proc.\ ICLR}, 2018.

\bibitem{emmons2020sparse}
S.~Emmons \textit{et al.}, ``Sparse graphical memory for robust planning,'' in \textit{Proc.\ NeurIPS}, 2020.

\bibitem{fonseca2022fsd50k}
E.~Fonseca \textit{et al.}, ``FSD50K: An open dataset of human-labeled sound events,'' \textit{IEEE/ACM Trans.\ Audio Speech Lang.\ Process.}, vol.~30, pp.~829--852, 2022.

\bibitem{kahneman2011thinking}
D.~Kahneman, \textit{Thinking, Fast and Slow}. Farrar, Straus and Giroux, 2011.

\bibitem{sharon2015conflict}
G.~Sharon \textit{et al.}, ``Conflict-based search for optimal multi-agent pathfinding,'' \textit{Artif.\ Intell.}, vol.~219, pp.~40--66, 2015.

\end{thebibliography}

% ═══════════════════════════════════════════════════════════════════════════
%  APPENDIX
% ═══════════════════════════════════════════════════════════════════════════

\appendix

\section{Neo4j Schema}

\begin{lstlisting}[style=cypher, caption={Neo4j Cypher schema}]
-- Landmark node
MERGE (l:Landmark {id: $id})
SET l.embedding       = $embedding,
    l.visitCount      = $visitCount,
    l.valueEstimate   = $valueEstimate,
    l.noveltyScore    = $noveltyScore,
    l.clusterId       = $clusterId

-- Transition edge
MATCH (src:Landmark {id: $srcId})
MATCH (tgt:Landmark {id: $tgtId})
MERGE (src)-[t:TRANSITION {action: $action}]->(tgt)
SET t.reward      = $reward,
    t.confidence  = $confidence,
    t.tdError     = $tdError

-- Constraints
CREATE CONSTRAINT IF NOT EXISTS
  FOR (l:Landmark) REQUIRE l.id IS UNIQUE
\end{lstlisting}

\section{Implementation Statistics}

\begin{table}[htbp]
\caption{Reference Implementation Size}
\centering
\begin{tabular}{@{}lr@{}}
\toprule
\textbf{Component} & \textbf{Lines of C\#} \\
\midrule
Abstractions (Interfaces)    & 120 \\
Models (Data Records)        & 200 \\
Neo4j Graph Memory           & 350 \\
Cartographer (Planner)       & 250 \\
Split-MNIST PoC              & 770 \\
Split-Audio PoC              & 550 \\
Program + Benchmark          & 190 \\
\midrule
\textbf{Total}               & \textbf{$\sim$2,430} \\
\bottomrule
\end{tabular}
\end{table}

Source: \url{https://github.com/Minu476/rich-learning} (Apache~2.0). Patent pending.

\end{document}
